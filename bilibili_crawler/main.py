import os
import time
import json
import random
import argparse
import schedule
from datetime import datetime
from bilibili_client import BilibiliClient
from bilibili_api import BilibiliAPI
from structure_tracker import StructureTracker
from danmaku_util import parse_danmaku_xml

log_dir = "bilibili_crawler/logs"
os.makedirs(log_dir, exist_ok=True)
log_file_path = os.path.join(log_dir, f"run_{datetime.now().strftime('%Y-%m-%d')}.log")


def log(msg):
    timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    full_msg = f"{timestamp} {msg}"
    print(full_msg)
    with open(log_file_path, "a", encoding="utf-8") as f:
        f.write(full_msg + "\n")


def parse_args():
    parser = argparse.ArgumentParser(description="Bilibili è§†é¢‘å¼¹å¹•ä¸‹è½½å™¨")
    parser.add_argument(
        "--days", type=int, default=10, help="ä¸‹è½½æœ€è¿‘å‡ å¤©çš„è§†é¢‘ï¼ˆé»˜è®¤10å¤©ï¼‰"
    )
    parser.add_argument(
        "--downloads",
        type=str,
        default="downloads",
        help="ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤ ./downloadsï¼‰",
    )
    parser.add_argument(
        "--save-xml", action="store_true", help="æ˜¯å¦ä¿å­˜åŸå§‹å¼¹å¹• XML æ–‡ä»¶"
    )
    return parser.parse_args()


def load_up_mids(file_path="bilibili_crawler/up_list.txt"):
    mids = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and line.isdigit():
                    mids.append(int(line))
    except FileNotFoundError:
        log(f"[ERROR] æ‰¾ä¸åˆ° UP ä¸»åˆ—è¡¨æ–‡ä»¶ï¼š{file_path}")
    return mids


def safe_sleep(min_sec=10.0, max_sec=30.0, reason="ç­‰å¾…ä¸­..."):
    """äººç±»åŒ– sleepï¼Œé˜²æ­¢è§¦å‘ B ç«™é£æ§"""
    duration = round(random.uniform(min_sec, max_sec), 2)
    print(f"ğŸ˜´ {reason}ï¼Œä¼‘çœ  {duration}s")
    time.sleep(duration)


# ä¸»ä»»åŠ¡
def download_videos():
    tracker = StructureTracker(DOWNLOAD_DIR)
    login_info = api.get_login_info()
    is_login = login_info.get("isLogin", False)
    if is_login:
        print(
            f"ğŸ˜„ ç™»å½•è´¦å·ï¼š{login_info.get('mid',0)}ï¼Œè´¦å·åï¼š{login_info.get('uname','æœªçŸ¥')}"
        )
    else:
        print("ğŸ˜¢ æœªç™»å½•çŠ¶æ€...")

    UP_MIDS = load_up_mids()
    for mid in UP_MIDS:
        try:
            log(f"[{datetime.now()}] æ­£åœ¨å¤„ç† UP ä¸»ï¼š{mid}")

            if is_login:
                uploader_info = api.get_uploader_info(mid)
                up_dir = os.path.join(DOWNLOAD_DIR, str(mid))
                os.makedirs(up_dir, exist_ok=True)
                with open(
                    os.path.join(up_dir, "info.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(uploader_info, f, ensure_ascii=False, indent=2)

            if is_login:
                videos = api.get_videos(mid)
            else:
                videos = api.get_videos_public(mid)

            for video in videos:
                try:
                    bvid = video["bvid"]
                    title = video["title"]
                    log(f"  â¬ ä¸‹è½½è§†é¢‘ï¼š{title} ({bvid})")

                    video_info = api.get_video_info(bvid)
                    cid = video_info["cid"]
                    danmaku_xml = api.get_danmaku_xml(cid)
                    if is_login:
                        summary = api.get_ai_summary(bvid, cid, mid)

                    save_path = os.path.join(DOWNLOAD_DIR, str(mid), bvid)
                    os.makedirs(save_path, exist_ok=True)

                    # ä¿å­˜è§†é¢‘ä¿¡æ¯
                    with open(
                        os.path.join(save_path, "video_info.json"),
                        "w",
                        encoding="utf-8",
                    ) as f:
                        json.dump(video_info, f, ensure_ascii=False)

                    # ä¿å­˜å¼¹å¹•
                    if args.save_xml:
                        with open(
                            os.path.join(save_path, "danmaku.xml"),
                            "w",
                            encoding="utf-8",
                        ) as f:
                            f.write(danmaku_xml)

                    # è§£æ danmaku.xml ä¸º JSON
                    try:
                        danmaku_json = parse_danmaku_xml(danmaku_xml)
                    except Exception as e:
                        log(
                            f"[âš ï¸ å¼¹å¹•è§£æå¤±è´¥] bvid={bvid} é”™è¯¯ï¼š{type(e).__name__}: {e}"
                        )
                        danmaku_json = []

                    # æ„å»ºç»Ÿä¸€ç»“æ„
                    combined_data = {
                        "bvid": bvid,
                        "cid": cid,
                        "videoData": video_info,
                        "danmakuData": danmaku_json,
                        "fetchtime": int(time.time()),
                    }

                    # ä¿å­˜ä¸º BVxxxxx.jsonï¼ˆæˆ– full_video.jsonï¼‰
                    with open(
                        os.path.join(save_path, f"{bvid}.json"), "w", encoding="utf-8"
                    ) as f:
                        json.dump(combined_data, f, ensure_ascii=False)

                    public_time = datetime.fromtimestamp(
                        video_info.get("pubdate", 0)
                    ).strftime("%Y-%m-%d %H:%M:%S")
                    download_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    # ä¿å­˜æ‘˜è¦å’Œä¸‹è½½æ—¶é—´
                    with open(
                        os.path.join(save_path, "summary.txt"), "w", encoding="utf-8"
                    ) as f:
                        f.write(f"è§†é¢‘æ ‡é¢˜ï¼š{title}\n")
                        f.write(f"å‘å¸ƒæ—¶é—´ï¼š{public_time}\n")
                        f.write(f"ä¸‹è½½æ—¶é—´ï¼š{download_time}\n")
                        if is_login:
                            f.write(f"AIæ‘˜è¦ï¼š{summary}\n")

                    tracker.update(
                        mid, bvid, title, public_time, download_time, save_path
                    )

                    safe_sleep(reason=f"å¤„ç†å®Œä¸€ä¸ªè§†é¢‘ï¼ˆbvid={bvid}ï¼‰åå†·å´")
                except Exception as e:
                    log(
                        f"[âŒ é”™è¯¯] å¤„ç† mid = {mid}, bvid = {bvid} æ—¶å‡ºé”™ï¼š{type(e).__name__}: {e}"
                    )
            safe_sleep(reason=f"å¤„ç†å®Œä¸€ä¸ªUPä¸»ï¼ˆmid={mid}ï¼‰åå†·å´")

        except Exception as e:
            log(f"[âŒ é”™è¯¯] å¤„ç† mid = {mid} æ—¶å‡ºé”™ï¼š{type(e).__name__}: {e}")

    tracker.save()
    print(f"ğŸ˜Š å®Œæˆä¸€è½®ä¸‹è½½ [{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]")


UP_MIDS = load_up_mids()
args = parse_args()
VIDEO_LOOKBACK_DAYS = args.days
DOWNLOAD_DIR = args.downloads

# client = BilibiliClient()
api = BilibiliAPI(BilibiliClient("bilibili_crawler/cookie.txt"))

if __name__ == "__main__":
    # å®šæ—¶ä»»åŠ¡ï¼šæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
    schedule.every(1).hours.do(download_videos)

    log("[å¯åŠ¨] æ­£åœ¨ç«‹å³æ‰§è¡Œé¦–æ¬¡ä»»åŠ¡...")

    download_videos()

    log("[å®ˆå€™] æ¯å°æ—¶è‡ªåŠ¨è½®è¯¢...")
    while True:
        schedule.run_pending()
        time.sleep(60)
