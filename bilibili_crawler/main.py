import os
import sys
import time
import json
import random
import argparse
import schedule
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
from wbi_sign import encWbi, getWbiKeys
from http_util import http_get, http_get_text


log_dir = "bilibili_crawler/logs"
os.makedirs(log_dir, exist_ok=True)
log_file_path = os.path.join(log_dir, f"run_{datetime.now().strftime('%Y-%m-%d')}.log")


def log(msg):
    timestamp = datetime.now().strftime("[%Y-%m-%d %H:%M:%S]")
    full_msg = f"{timestamp} {msg}"
    print(full_msg)
    with open(log_file_path, "a", encoding="utf-8") as f:
        f.write(full_msg + "\n")


def parse_args():
    parser = argparse.ArgumentParser(description="Bilibili è§†é¢‘å¼¹å¹•ä¸‹è½½å™¨")
    parser.add_argument(
        "--days", type=int, default=10, help="ä¸‹è½½æœ€è¿‘å‡ å¤©çš„è§†é¢‘ï¼ˆé»˜è®¤10å¤©ï¼‰"
    )
    return parser.parse_args()


def load_up_mids(file_path="bilibili_crawler/up_list.txt"):
    mids = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and line.isdigit():
                    mids.append(int(line))
    except FileNotFoundError:
        log(f"[ERROR] æ‰¾ä¸åˆ° UP ä¸»åˆ—è¡¨æ–‡ä»¶ï¼š{file_path}")
    return mids


# UPä¸» mid åˆ—è¡¨
UP_MIDS = load_up_mids()
args = parse_args()
VIDEO_LOOKBACK_DAYS = args.days

# ç¼“å­˜ä¸€æ¬¡ Wbi keysï¼Œé¿å…å¤šæ¬¡è¯·æ±‚
IMG_KEY, SUB_KEY = getWbiKeys()


def safe_sleep(min_sec=10.0, max_sec=30.0, reason="ç­‰å¾…ä¸­..."):
    """äººç±»åŒ– sleepï¼Œé˜²æ­¢è§¦å‘ B ç«™é£æ§"""
    duration = round(random.uniform(min_sec, max_sec), 2)
    print(f"ğŸ˜´ {reason}ï¼Œä¼‘çœ  {duration}s")
    time.sleep(duration)


def parse_danmaku_xml(xml_text):
    danmakus = []
    try:
        root = ET.fromstring(xml_text)
        for d in root.findall("d"):
            p = d.get("p")
            if not p:
                continue
            parts = p.split(",")
            if len(parts) < 8:
                continue
            danmaku = {
                "progress": int(float(parts[0]) * 1000),
                "mode": int(parts[1]),
                "fontsize": int(parts[2]),
                "color": int(parts[3]),
                "ctime": int(parts[4]),
                "pool": int(parts[5]),
                "midHash": parts[6],
                "dmid": parts[7],
                "weight": int(parts[8]) if len(parts) > 8 else 0,
                "content": (d.text or "").strip(),
            }
            danmakus.append(danmaku)
        return danmakus
    except Exception as e:
        log(f"[âš ï¸ å¼¹å¹•è§£æå¤±è´¥] {e}")
        return []


def build_full_video_json(bvid, cid, video_info, danmaku_json):
    return {
        "bvid": bvid,
        "cid": cid,
        "videoData": video_info,
        "danmakuData": danmaku_json,
    }


def check_login():
    res = http_get("https://api.bilibili.com/x/web-interface/nav", desc="éªŒè¯ç™»å½•çŠ¶æ€")
    is_login = res.get("data", {}).get("isLogin", False)
    if is_login:
        log("âœ… å½“å‰å¤„äºç™»å½•çŠ¶æ€ï¼ŒCookie æœ‰æ•ˆ")
    else:
        log("âŒ å½“å‰æœªç™»å½•ï¼Œè¯·æ£€æŸ¥ cookie.txt")
        sys.exit(1)


# è·å– UP ä¸»ä¿¡æ¯
def get_uploader_info(mid):
    url = "https://api.bilibili.com/x/space/wbi/acc/info"
    params = {"mid": str(mid)}
    signed_params = encWbi(params, IMG_KEY, SUB_KEY)
    res = http_get(url, params=signed_params, desc=f"è·å– UP ä¸»ä¿¡æ¯ mid={mid}")
    return res["data"]


# è·å– UP ä¸»è§†é¢‘åˆ—è¡¨
def get_videos(mid):
    videos = []
    page = 1
    cutoff_time = datetime.now() - timedelta(days=VIDEO_LOOKBACK_DAYS)

    while True:
        url = "https://api.bilibili.com/x/space/wbi/arc/search"
        params = {"mid": str(mid), "pn": str(page), "ps": "30", "order": "pubdate"}
        signed_params = encWbi(params, IMG_KEY, SUB_KEY)
        res = http_get(url, signed_params, desc="è·å–è§†é¢‘åˆ—è¡¨")

        if res["code"] != 0 or "list" not in res["data"]:
            print(res)
            break

        for v in res["data"]["list"]["vlist"]:
            pubdate = datetime.fromtimestamp(v["created"])
            if pubdate < cutoff_time:
                return videos
            videos.append(v)

        if not res["data"]["list"]["vlist"]:
            break
        page += 1

    return videos


# è·å–è§†é¢‘è¯¦ç»†ä¿¡æ¯
def get_video_info(bvid):
    url = f"https://api.bilibili.com/x/web-interface/view"
    res = http_get(url, {"bvid": bvid}, desc="è·å–è§†é¢‘ä¿¡æ¯")
    return res["data"]


# è·å–å¼¹å¹• XMLï¼ˆæ–°ç‰ˆ APIï¼‰
def get_danmaku_xml(cid):
    url = f"https://api.bilibili.com/x/v1/dm/list.so?oid={cid}"
    return http_get_text(url, desc="è·å–å¼¹å¹•XML")


# è·å– AI æ‘˜è¦ï¼ˆWbi ç­¾åï¼‰
def get_ai_summary(bvid, cid, mid):
    url = "https://api.bilibili.com/x/web-interface/view/conclusion/get"
    params = {"bvid": bvid, "cid": str(cid), "up_mid": str(mid)}
    signed_params = encWbi(params, IMG_KEY, SUB_KEY)
    res = http_get(url, signed_params, desc="è·å–AIæ‘˜è¦")
    try:
        return res["data"]["model_result"].get("summary", "æ— æ‘˜è¦")
    except:
        return "æ— æ‘˜è¦"


# ä¸»ä»»åŠ¡
def download_videos():
    UP_MIDS = load_up_mids()
    for mid in UP_MIDS:
        try:
            log(f"[{datetime.now()}] æ­£åœ¨å¤„ç† UP ä¸»ï¼š{mid}")

            # ä¿å­˜ UP ä¸»ä¿¡æ¯
            uploader_info = get_uploader_info(mid)
            up_dir = os.path.join("downloads", str(mid))
            os.makedirs(up_dir, exist_ok=True)
            with open(os.path.join(up_dir, "info.json"), "w", encoding="utf-8") as f:
                json.dump(uploader_info, f, ensure_ascii=False, indent=2)

            videos = get_videos(mid)

            for video in videos:
                bvid = video["bvid"]
                title = video["title"]
                pub_time = datetime.fromtimestamp(video["created"]).strftime(
                    "%Y-%m-%d %H:%M:%S"
                )
                log(f"  â¬ ä¸‹è½½è§†é¢‘ï¼š{title} ({bvid})")

                video_info = get_video_info(bvid)
                cid = video_info["cid"]
                danmaku_xml = get_danmaku_xml(cid)
                summary = get_ai_summary(bvid, cid, mid)

                save_path = os.path.join("downloads", str(mid), bvid)
                os.makedirs(save_path, exist_ok=True)

                # ä¿å­˜è§†é¢‘ä¿¡æ¯
                with open(
                    os.path.join(save_path, "video_info.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(video_info, f, ensure_ascii=False, indent=2)

                # ä¿å­˜å¼¹å¹•
                with open(
                    os.path.join(save_path, "danmaku.xml"), "w", encoding="utf-8"
                ) as f:
                    f.write(danmaku_xml)

                # è§£æ danmaku.xml ä¸º JSON
                danmaku_json = parse_danmaku_xml(danmaku_xml)

                # æ„å»ºç»Ÿä¸€ç»“æ„
                combined_data = build_full_video_json(
                    bvid, cid, video_info, danmaku_json
                )

                # ä¿å­˜ä¸º danmaku.jsonï¼ˆæˆ– full_video.jsonï¼‰
                with open(
                    os.path.join(save_path, f"{bvid}.json"), "w", encoding="utf-8"
                ) as f:
                    json.dump(combined_data, f, ensure_ascii=False, indent=2)

                # ä¿å­˜æ‘˜è¦å’Œä¸‹è½½æ—¶é—´
                with open(
                    os.path.join(save_path, "summary.txt"), "w", encoding="utf-8"
                ) as f:
                    f.write(
                        f"ä¸‹è½½æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                    )
                    f.write(f"AIæ‘˜è¦ï¼š{summary}\n")

                safe_sleep(reason=f"å¤„ç†å®Œä¸€ä¸ªè§†é¢‘ï¼ˆbvid={bvid}ï¼‰åå†·å´")

            safe_sleep(reason=f"å¤„ç†å®Œä¸€ä¸ªUPä¸»ï¼ˆmid={mid}ï¼‰åå†·å´")

        except Exception as e:
            log(f"[âŒ é”™è¯¯] å¤„ç† mid={mid} æ—¶å‡ºé”™ï¼š{e}")


# å®šæ—¶ä»»åŠ¡ï¼šæ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
schedule.every(1).hours.do(download_videos)

log("[å¯åŠ¨] æ­£åœ¨ç«‹å³æ‰§è¡Œé¦–æ¬¡ä»»åŠ¡...")
check_login()
download_videos()

log("[å®ˆå€™] æ¯å°æ—¶è‡ªåŠ¨è½®è¯¢...")
while True:
    schedule.run_pending()
    time.sleep(60)
